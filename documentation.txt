# -*-Sh-*-       

############################################################################################################
# January 20, 2017
# This is a documentation/readme file for where all the scripts live and the order of analyses performed.
############################################################################################################                        

# to run the code, the following symbolic links must be made to the home dir:
ben_nandita_hmp_analysis
ben_nandita_hmp_data
ben_nandita_hmp_scripts
tmp_intermediate_files

#update $PYTHONPATH with ben_nandita_hmp_scripts path                                             
#update $PATH with ben_nandita_hmp_scripts path because it contains the error c++ code       

#####################
# HMP data download #
#####################
# Obtain HMP shotgun data (see Stephen's dir)
# Obtain HMP metadata from Westway (HMP_ids.txt)                                                                                                                                                  
mysql -u ngarud -p -e"select subject_id, sample_id, run_accession, country, continent from MetaQuery.run_to_study a join MetaQuery.run_to_sample b using(run_accession) join MetaQuery.sample_to_subject c using(sample_id) join MetaQuery.subject_attributes d using(subject_id) where study_id ='HMP'" > HMP_ids.txt
d
# HMP meta data download for parsing time data:
mysql -u ngarud -p -e"select run_accession,sample_accession,sample_alias from SRAdb.sra " > sra_mapping.txt


##########################################################
# Concatenate the technical replicates fastq files 
# (increases power, especially for samples with low depth. 
# Eliminates need later on to merge tech. reps). 
###########################################################
python concatenate_fastq_technical_replicates.py
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/concatenate_fastq_technical_replicates


#############
# Run MIDAS #
#############
# need a list of sample IDs
sed '1d' ~/ben_nandita_hmp_scripts/HMP_ids.txt | cut -f2 | sort | uniq > /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/sampleIDs.txt

# MIDAS: run the species, SNPs, CNVs modules all together linearly for each sample ID (cuts down on waiting time)
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_species_snps_cnvs

# run MIDAS on Kuleshov data
# accession list: /netapp/home/ngarud/shattuck/metagenomic_fastq_files/Kuleshov/accession_list.txt
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_Kuleshov_MIDAS_species_snps_cnvs_sample1
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_Kuleshov_MIDAS_species_snps_cnvs_sample2

# MIDAS: run the merge step for HMP and kuleshov combined (here we need to define parameters)
# the output goes to ~/ben_nandita_hmp_data/snps, ~/ben_nandita_hmp_data/cnvs, ~/ben_nandita_hmp_data/species
# note, I have copied the kuleshov sample to the HMP dir. 
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_species_merge
#qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_SNPs_merge_5_3
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_SNPs_merge_5_3_core_and_variable_samples_combined
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_CNVs_merge



# bzip2 -- need to compress the files to run it through the postprocessing code below
bzip2 snps/${species}/snps_depth.txt
bzip2 snps/${species}/snps_info.txt
bzip2 snps/${species}/snps_ref_freq.txt
bzip2 snps/${species}/snps_alt_allele.txt
bzip2 genes/${species}/genes_copynum.txt
bzip2 genes/${species}/genes_depth.txt
bzip2 genes/${species}/genes_presabs.txt
bzip2 genes/${species}/genes_reads.txt

bzip2 species/coverage.txt 
bzip2 species/count_reads.txt 
bzip2 species/relative_abundnace.txt 
bzip2 species/species_prevalence.txt 

qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_snps
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_genes
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_species


# NOTE: all of these files are now located in the following dirs:
genes_tech_reps_combined
pecies_tech_reps_combined
snps_tech_reps_combined

###########################################################
# Concatenate the sample replicates fastq files 
# (increases power, especially for samples with low depth. 
# Eliminates need later on to merge sample reps). 
###########################################################

# generate a list of subject_ids
python ~/ben_nandita_hmp_scripts/get_list_of_subject_ids.py

python ~/ben_nandita_hmp_scripts/concatenate_fastq_sample_replicates_from_same_visno.py $subject_id
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/concatenate_fastq_sample_replicates

# need a list of combined sample IDs to run MIDAS on 
ls ~/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/joined_fastq_files_hmp_combine_sample_reps | cut -f1 -d'_' | sort | uniq > ~/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/combined_sampleIDs.txt

# MIDAS: run the species, SNPs, CNVs modules all together linearly for each sample ID (cuts down on waiting time)
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_species_snps_cnvs_samples_combined

# MIDAS: run the merge step for HMP and kuleshov combined (here we need to define parameters)
# the output goes to ~/ben_nandita_hmp_data/snps, ~/ben_nandita_hmp_data/cnvs, ~/ben_nandita_hmp_data/species
# note, I have copied the kuleshov sample to the HMP dir. 

# get a list of all the sample paths:
python ~/ben_nandita_hmp_scripts/fastq_samples_without_visno_replicate.py 

# combined samples:
ls -d -1 /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_samples_combined_output/** >> /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/sample_paths.txt

# Kuleshov:
echo '/netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_output/SRR2822459' >> /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/sample_paths.txt 

# SEE BELOW FOR QIN

# merge samples without visno replicates
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_species_merge_samples_combined
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_SNPs_merge_5_3_core_and_variable_samples_combined
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_CNVs_merge_samples_combined 

#bzip everything
cd /netapp/home/ngarud/shattuck/BenNanditaProject/ben_nandita_hmp_data/snps/
ls > /netapp/home/ngarud/shattuck/BenNanditaProject/ben_nandita_hmp_data/snps/species_snps.txt
cd /netapp/home/ngarud/shattuck/BenNanditaProject/ben_nandita_hmp_data/genes/
ls > /netapp/home/ngarud/shattuck/BenNanditaProject/ben_nandita_hmp_data/genes/species_genes.txt

qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_snps
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_genes
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_species

#########################################################
# Run MIDAS using different MAPID values for bowtie     #
#########################################################
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_snps_mapsid_0.90_test
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_snps_mapsid_0.92_test

# run samtools to create a bai file

for sample in 700037453  700037539; do 
    for mapid in 0.90 0.92; do
	cd /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_mapid_test_${mapid}/${sample}/snps/temp
	nohup nice samtools index genomes.bam  genomes.bam.bai &
    done
done


for sample in 700037453  700037539; do 
    cd /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_output/${sample}/snps/temp
    nohup nice samtools index genomes.bam  genomes.bam.bai &
done

for sample in 700037453  700037539; do
    cd /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_output/${sample}/genes/temp
    nohup nice samtools index pangenomes.bam  pangenomes.bam.bai &
done
# Grab the FASTA sequence and coords on which the marker genes map:
#for gene in 411479.10.peg.1275 411479.10.peg.1736 411479.10.peg.250 411479.10.peg.3376 411479.10.peg.340 411479.10.peg.3401 411479.10.peg.3504 411479.10.peg.376 411479.10.peg.3804 411479.10.peg.3997 411479.10.peg.4021 411479.10.peg.513 411479.10.peg.797 411479.10.peg.89; do
for gene in 411479.10.peg.250 411479.10.peg.3376 411479.10.peg.340 411479.10.peg.3401 411479.10.peg.3504 411479.10.peg.376 411479.10.peg.3804 411479.10.peg.3997 411479.10.peg.4021 411479.10.peg.513 411479.10.peg.797 411479.10.peg.89; do
  
   echo $gene
    bzcat ~/ben_nandita_hmp_data/snps/Bacteroides_uniformis_57318/snps_info.txt.bz2 | grep -w $gene | head -1 | cut -f1 
    bzcat ~/ben_nandita_hmp_data/snps/Bacteroides_uniformis_57318/snps_info.txt.bz2 | grep -w $gene | tail -1 | cut -f1
done

###########################################################################
# Run MIDAS on HMP data using only B. uniformis and A. putredinis genomes #
# Do we recover reads that are lost?
###########################################################################

# create folders and a copy of the species output for all the HMP samples for the readstealing test

while read path; do
    sample=`echo $path | cut -f9 -d'/'`
    echo $sample
    #mkdir /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_read_stealing_test/$sample
    cp -R  $path/species /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_read_stealing_test/$sample 
done < ~/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/sample_paths.txt

# run species module on B. unif -- the labels dont seem to add up.
# run midas on the problematic sample only
path=~/BenNanditaProject/MIDAS_intermediate_files_hmp/joined_fastq_files_hmp_combine_sample_reps

for sample in 700107189c 700038761c 700037284c 700037123c 700034254c; do
#sample=700037738c
OUTDIR=~/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_read_stealing_test/Bacteroides_uniformis_57318/${sample}

nohup nice run_midas.py species $OUTDIR -1 ${path}/${sample}_1.fastq.gz -2 ${path}/${sample}_2.fastq.gz &

done


qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_species_B_unif_only

# Run the gene module of MIDAS using only B. unif and A. putredinis genomes
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_genes_B_unif_only
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_genes_A_put_only

# Run the snp module of MIDAS using only B. unif and A. putredinis genomes
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_snps_B_unif_only
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_snps_A_put_only

# merge 

for species in Alistipes_putredinis_61533 Bacteroides_uniformis_57318; do
OUTDIR=~/ben_nandita_hmp_data/read_stealing_test/${species}

nohup nice merge_midas.py species $OUTDIR/species -i ~/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_read_stealing_test/${species}/ -t dir  >& $OUTDIR/species/species.log &

nohup nice merge_midas.py snps $OUTDIR/snps -i ~/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_read_stealing_test/${species}/ -t dir --sample_depth 5 --site_depth 3 --min_samples 1 --max_species 150 --site_prev 0.0 --threads 10 >& $OUTDIR/snps/snps.log &

nohup nice merge_midas.py genes $OUTDIR/genes -i ~/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_read_stealing_test/${species}/ -t dir --sample_depth 10 --min_samples 1 --max_species 150 >& $OUTDIR/genes/genes.log &

done 

#bzip
for species in Alistipes_putredinis_61533 Bacteroides_uniformis_57318; do 

dir=ben_nandita_hmp_data/read_stealing_test/${species}/snps/${species}

bzip2 ${dir}/snps_depth.txt &
bzip2 ${dir}/snps_info.txt &
bzip2 ${dir}/snps_ref_freq.txt &
bzip2 ${dir}/snps_alt_allele.txt &

dir=ben_nandita_hmp_data/read_stealing_test/${species}/genes/${species}

bzip2 ${dir}/genes_copynum.txt &
bzip2 ${dir}/genes_depth.txt &
bzip2 ${dir}/genes_presabs.txt &
bzip2 ${dir}/genes_reads.txt &

dir=ben_nandita_hmp_data/read_stealing_test/${species}/species

bzip2 ${dir}/coverage.txt &
bzip2 ${dir}/count_reads.txt &
bzip2 ${dir}/relative_abundance.txt &
bzip2 ${dir}/species_prevalence.txt &

done

# postprocess (change config file fos that data_directory points to ~/ben_nandita_hmp_data/read_stealing_test/Alistipes_putredinis_61533
nohup nice postprocess_midas_data.py Alistipes_putredinis_61533 &
nohup nice postprocess_midas_data.py Bacteroides_uniformis_57318 &

########################################################
# Run MIDAS on Qin et al. healthy samples              #
########################################################        
mysql -u ngarud -p -e"select subject_id, sample_id, run_accession, country, continent, t2d from MetaQuery.run_to_study a join MetaQuery.run_to_sample b using(run_accession) join MetaQuery.sample_to_subject c using(sample_id) join MetaQuery.subject_attributes d using(subject_id) where study_id ='T2D'" > Qin_2012_ids_all.txt

mysql -u ngarud -p -e"select subject_id, sample_id, run_accession, country, continent, t2d from MetaQuery.run_to_study a join MetaQuery.run_to_sample b using(run_accession) join MetaQuery.sample_to_subject c using(sample_id) join MetaQuery.subject_attributes d using(subject_id) where study_id ='T2D' and healthy=0" > Qin_2012_ids_healthy.txt

cat ~/ben_nandita_hmp_data/Qin_2012_ids_healthy.txt | cut -f3 > ~/BenNanditaProject/MIDAS_intermediate_files_Qin_2012/sample_IDs_healthy.txt

# run midas
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_Qin_MIDAS_species_snps_cnvs
# Merge data (with HMP and Kuleshov)
# get a list of all the sample paths:
python ~/ben_nandita_hmp_scripts/fastq_samples_without_visno_replicate.py 

# combined samples:
ls -d -1 /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_samples_combined_output/** >> /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/sample_paths.txt

# Kuleshov:
echo '/netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_output/SRR2822459' >> /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/sample_paths.txt 

# Qin:
ls -d -1 /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_Qin_2012/MIDAS_1.2.2_output/** > /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_Qin_2012/sample_paths_Qin.txt

cat /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_hmp/sample_paths.txt /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_Qin_2012/sample_paths_Qin.txt > /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_Qin_2012/sample_paths_hmp_Kuleshov_Qin.txt

# merge samples without visno replicates
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_Qin_MIDAS_species_merge_samples_combined
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_Qin_MIDAS_SNPs_merge_5_3_core_and_variable_samples_combined
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_Qin_MIDAS_CNVs_merge_samples_combined 

qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_Qin_MIDAS_CNVs_1_merge_samples_combined 
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_Qin_MIDAS_CNVs_5_merge_samples_combined 

#bzip everything
cd /netapp/home/ngarud/shattuck/BenNanditaProject/ben_nandita_hmp_data/Qin_HMP/snps_Qin_HMP_samples_combined/
ls > /netapp/home/ngarud/shattuck/BenNanditaProject/ben_nandita_hmp_data/Qin_HMP/snps_Qin_HMP_samples_combined/species_snps.txt

cd /netapp/home/ngarud/shattuck/BenNanditaProject/ben_nandita_hmp_data/Qin_HMP/genes_Qin_HMP_samples_combined/
ls > /netapp/home/ngarud/shattuck/BenNanditaProject/ben_nandita_hmp_data/Qin_HMP/genes_Qin_HMP_samples_combined/species_genes.txt

qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_snps_Qin_HMP
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_genes_Qin_HMP
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_species_Qin_HMP


####################################
# Run MIDAS on FMT (Li 2016) data  #
####################################                                                 

# first, merge all the replicates for paired-end files so that we have a single forward and single reverse file.  

cat ~/BenNanditaProject/MIDAS_intermediate_files_Li_2016_FMT/Li_2016_FMT_run_accessions.txt | grep paired | cut -f1,3 > ~/BenNanditaProject/MIDAS_intermediate_files_Li_2016_FMT/Li_2016_FMT_paired_accessions.txt

cat ~/BenNanditaProject/MIDAS_intermediate_files_Li_2016_FMT/Li_2016_FMT_run_accessions.txt | grep paired | cut -f3 | sort | uniq > ~/BenNanditaProject/MIDAS_intermediate_files_Li_2016_FMT/Li_2016_FMT_paired_samples_only.txt

~/BenNanditaProject/ben_nandita_hmp_scripts/concatenate_Li_2016_FMT_fastq.py $sample

qsub ~/ben_nandita_hmp_scripts/qsub_scripts/concatenate_Li_2016_FMT_fastq
# need to do this one separately because the donor shows up 3x with different ids
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/concatenate_Li_2016_FMT_fastq_DON_11

# run MIDAS:
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_Li_2016_FMT_MIDAS_species_snps_cnvs

# in this merge I will include HMP, Kuleshov, Qin, and FMT:
# see above for generation of this file. Now add to it:
path_file=/netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_Qin_2012/sample_paths_hmp_Kuleshov_Qin.txt

ls -d -1 /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_Li_2016_FMT/MIDAS_1.2.2_output/** > /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_Li_2016_FMT/sample_paths_Li_2016_FMT.txt

cat $path_file /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_Li_2016_FMT/sample_paths_Li_2016_FMT.txt > /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_Li_2016_FMT/sample_paths_HMP_Kuleshov_Qin_Li.txt 

#merge
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_Kuleshov_Qin_Li_species
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_Kuleshov_Qin_Li_snps
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_Kuleshov_Qin_Li_genes

#bzip
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_snps_HMP_Kuleshov_Qin_Li
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_genes_HMP_Kuleshov_Qin_Li
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_species_HMP_Kuleshov_Qin_Li

# Do another merge without the HMP, Kuleshov, Qin files. 
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_Li_FMT_merge_species
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_Li_FMT_merge_snps
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_Li_FMT_merge_genes
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_Li_FMT_merge_genes_5
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_Li_FMT_merge_genes_1
# bzip
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_snps_Li_FMT
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_genes_Li_FMT
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_species_Li_FMT

ls /netapp/home/ngarud/ben_nandita_hmp_data/Li_FMT/genes_1 > /netapp/home/ngarud/ben_nandita_hmp_data/Li_FMT/genes_1/species_genes.txt
ls /netapp/home/ngarud/ben_nandita_hmp_data/Li_FMT/genes_5 > /netapp/home/ngarud/ben_nandita_hmp_data/Li_FMT/genes_5/species_genes.txt

qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_genes_5_Li_FMT
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/bzip_genes_1_Li_FMT

tar -cf Li_FMT.tar Li_FMT
tar -cf genes_1.tar genes_1
tar -cf genes_5.tar genes_5


###########################
# Postprocessing of MIDAS #
###########################
# The purpose of this step is to and assign a probability tvhat a site's genootype is real or an error. 

# calculate_marker_gene_coverage.py -- (another possibility is to change to using the CNV output)
    #output: marker_coverage.txt.bz2

# calculate_coverage_distribution.py -- This filters which genes have less than or greater than 2-fold difference from the median coverage in the data set. We want to filter these genes out (i.e. not enough info to call a SNP or too much coverage means that there might be a CNV)
    #output: sample_coverage_distribution.txt.bz2
    #output: sample_gene_coverage.txt.bz2
 
# calculate_error_pvalues.py -- (this uses the cpp code Ben wrote)
    # output: sample_annotated_snps.txt.bz2 

#compile the error_pvalues cpp code as follows: 
g++ -std=c++11 -O3 *.cpp -o annotate_pvalue

# python script to run the above:
postprocess_midas_data.py $species

# bash scripts to run the above:
for species in Prevotella_copri_61740 Butyrivibrio_crossotus_61674 Bacteroides_uniformis_57318 Roseburia_intestinalis_56239 Eubacterium_eligens_61678; do     
    nohup nice bash ~/ben_nandita_hmp_scripts/bash_scripts/postprocess_midas_data.sh $species & > ~/BenNanditaProject/nohup_out/nohup_out_$species.txt
done

# qsub script to run the above:
python ~/ben_nandita_hmp_scripts/print_good_species_list.py name> ~/tmp_intermediate_files/tmp_species_list.txt

qsub ~/ben_nandita_hmp_scripts/qsub_scripts/run_postprocessing_scripts 

# run the next three scripts too
# Calculate substitution rates for the most prevalent species
species=Bacteroides_uniformis_57318
python ~/ben_nandita_hmp_scripts/calculate_substitution_rates.py --species $species

# Calculate linkage disequilibria for the most prevalent species
python ~/ben_nandita_hmp_scripts/calculate_linkage_disequilibria.py --species $species

# Calculate temporal changes for the most prevalent species
python ~/ben_nandita_hmp_scripts/calculate_temporal_changes.py --species $species

qsub ~/ben_nandita_hmp_scripts/qsub_scripts/run_postprocessing_scripts_part2


####################
# Analysis scripts #
####################

# Compute and plot diversity statistics (the key is that computations AND plotting are both done in the same piece of code). 
  # e.g.:
  # plot_hmp_metadata.py
  # plot_coverage_distribution.py 
  # plot_pooled_sfs.py 
  # plot_spatial_pi.py
  # plot_pNpS_vs_pi.py
  # plot_gene_ld.py
  # plot_pi_distribution.py
  # print_distance_matrix.py

  # plot_within_sfs.py (problem with from calculate_pi_matrix import calculate_self_pis)

for species in Prevotella_copri_61740 Butyrivibrio_crossotus_61674 Bacteroides_uniformis_57318 Roseburia_intestinalis_56239 Eubacterium_eligens_61678; do     
    nohup nice bash ~/projectBenNandita/bash_scripts/run_diversity_scripts.sh $species &
done

# other code that is imported into above code
  #parse_midas_data.py -- Can be called using 'import'. Is called in all the postprocesing of MIDAS scripts. Sometimes also called in the computation of diversity scripts.
     # defines where the data directory lives (default_directory_prefix)
     # parses the meta data
     # parses the output of the postprocessing step. 
     # reads in the original MIDAS data

     # parse_snps -- Loads list of SNPs and counts of target sites from annotated SNPs file

  #diversity_utils.py   -- this consists of several definitions that can be called using 'import' in other python scripts. E.g. see plot_pNpS_vs_pi.py
                      # -- is called in all the computation of diversity statistics
     # some functions include:



####################################
# Feb 6, 2017                      #
# code up alpha and beta diversity #
####################################

#alpha diversity= shannon index
#beta diversity=gamma_diversity/alpha_diversity

Rscript ~/projectBenNandita/plot_alpha_beta_diversity.R


#################################################
# Feb 7, 2017                                   #
# Make a plot for # of genes vs piS per sample  #
#################################################
# ben has coded up piS per sample
# I should reuse this + write a piece of code for # of genes per sample. 

# The thing is that I will be merging the fastq files upstream, so spending the time merging right now doesn't make sense. 

# once I have been able to run MIDAS on the merged dat, I can make this plot. This takes advantage of merged gene presabs etc. as well as Ben's code for computing piS per sample. 

~/ben_nandita_hmp_data/species/relative_abundance.txt.bz2
~/projectBenNandita/loop_over_technical_replicates.py


####################
# Haplotype plots  #
####################
# we want to make haplotype plots 

# cluster by H12 algorithm

# plot using haplotype info
python ~/ben_nandita_hmp_scripts/plot_gene_haplotypes.py $species --chunk-size 100000 --debug 

# later: 
# order the haplotypes based on pi_S. 


###################################################
# plot LD for different species on the same plot  #
###################################################

# condition on allele frequency:
plot_gene_ld_condition_freq.py
#plot LD for multiple species on the same plot
python plot_gene_ld_multispecies.py

# LD for syn vs nonsyn rare vs common alleles
plot_gene_ld_condition_freq_syn_nonsyn.py

#################################
# Use Rphylip to create a tree  #
#################################
species=Bacteroides_uniformis_57318  
python ~/projectBenNandita/print_distance_matrix.py $species > ~/ben_nandita_hmp_analysis/fst.dist_${species}

# the Rphylip not needed ATM:
#R script to plot a tree
install.packages("Rphylip")
install.packages("phylotools")

# how to install phylip:
cd ~/software
wget http://evolution.gs.washington.edu/phylip/download/phylip-3.696.tar.gz
cd /home/ngarud/software/phylip-3.696/src
make -f Makefile.unx install


     python ~projectBenNandita/plot_tree.py $species

##########################################
# blast metaphlan2 genes against patric: #
##########################################

# need to run this on chestnut -- doesn't seem to work on chef (but it is fast). 
while read species; do
    bash ~/projectBenNandita/bash_scripts/metaphlan2_blast.sh $species
done < ~/ben_nandita_hmp_data/snps/species_snps.txt


###################################################################
# read in a list of genes from metaphlan2  to plot piN/piS vs piS #
###################################################################

genes=/pollard/shattuck0/ngarud/BenNanditaProject/metaphlan2_to_patric/blast/Bacteroides_uniformis_57318_metaphlan2_genes_mapped.txt

species=Bacteroides_uniformis_57318
python ~/projectBenNandita/plot_pNpS_vs_pi_metaphlan2_genes.py $species


#############################################
# Time series: How many fixations vs time?  #
#############################################

species=Bacteroides_uniformis_57318  
species=Bacteroides_vulgatus_57955
python ~/ben_nandita_hmp_scripts/plot_fixations_vs_time.py $species


############
# Run analysis scripts
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/run_analysis_scripts

# for some reason I get 'Illegal instruction' for species with higher coverages. I can run the scripts fine out here, though. 

head ~/tmp_intermediate_files/tmp_species_list.txt > ~/tmp_intermediate_files/tmp_species_list_top10.txt

head -20 ~/tmp_intermediate_files/tmp_species_list.txt | tail -10 > ~/tmp_intermediate_files/tmp_species_list_next10.txt

head -30 ~/tmp_intermediate_files/tmp_species_list.txt > ~/tmp_intermediate_files/tmp_species_list_top30.txt


#####################
# Fixations vs time

while read species; do
    nohup nice python ~/ben_nandita_hmp_scripts/plot_fixations_vs_time_snps_genes.py $species --chunk-size 100000 &
done <  ~/tmp_intermediate_files/tmp_species_list_top30.txt  
#done < ~/tmp_intermediate_files/tmp_species_list_top10.txt

##########################################
# piS vs time
# Q: do haploids ever become polyploids?
#########################################
while read species; do
    nohup nice python ~/ben_nandita_hmp_scripts/plot_piS_ordered_by_time.py $species --chunk-size 100000 &
done <~/tmp_intermediate_files/tmp_species_list_top30.txt


###################################################
# SFS vs time
# Q: What do the two SFSs look like side by side?
# What does the 2D sfs look like?
##################################################
while read species; do
nohup nice python ~/ben_nandita_hmp_scripts/plot_within_sfs_ordered_by_time.py $species --chunk-size 100000 &
done <~/tmp_intermediate_files/tmp_species_list_top30.txt 


###################################################
# Kegg pathway analysis
###################################################

# cut all the Kegg pathway IDs and patric genome IDs 

# make a list of patric genomes. 
ls /pollard/shattuck0/snayfach/databases/PATRIC/genomes/ > ~/ben_nandita_hmp_data/patric_genomes.txt

while read PATRIC; do 
  file=`ls /pollard/shattuck0/snayfach/databases/PATRIC/genomes/${PATRIC}/${PATRIC}.PATRIC.features.tab*`

  if file --mime-type "$file" | grep -q gzip$; then
      zcat /pollard/shattuck0/snayfach/databases/PATRIC/genomes/${PATRIC}/${PATRIC}.PATRIC.features.tab | cut -f6,21 > ~/ben_nandita_hmp_data/kegg/${PATRIC}.kegg.txt
  else
      cat /pollard/shattuck0/snayfach/databases/PATRIC/genomes/${PATRIC}/${PATRIC}.PATRIC.features.tab | cut -f6,21 > ~/ben_nandita_hmp_data/kegg/${PATRIC}.kegg.txt  
	  
  fi
done < ~/ben_nandita_hmp_data/patric_genomes.txt


nohup nice bash ~/ben_nandita_hmp_scripts/bash_scripts/cut_kegg_pathway.sh &

#  bzip all the files
while read PATRIC; do
    nohup nice bzip2 ~/ben_nandita_hmp_data/kegg/${PATRIC}.kegg.txt &
done < ~/ben_nandita_hmp_data/patric_genomes.txt  

# load in the kegg pathways

plot_kegg_pathway_histogram.py


############################
while read species; do
nohup nice python ~/ben_nandita_hmp_scripts/plot_fixations_snps_vs_pi.py $species --chunk-size 100000 &
done <~/tmp_intermediate_files/tmp_species_list_top30.txt 


while read species; do
nohup nice python ~/ben_nandita_hmp_scripts/plot_kegg_pathway_histogram.py $species --chunk-size 100000 &
done <~/tmp_intermediate_files/tmp_species_list_top30.txt 


while read species; do
nohup nice python ~/ben_nandita_hmp_scripts/plot_kegg_pi_distribution.py $species --chunk-size 100000 &
done <~/tmp_intermediate_files/tmp_species_list.txt 


###########################
# April 30
# read in the patric genes for ABX resistance
genome=1154836.3
path=/pollard/shattuck0/snayfach/databases/PATRIC/genomes/${genome}/${genome}.PATRIC.spgene.tab.gz

ls /pollard/shattuck0/snayfach/databases/PATRIC/genomes/ > ~/ben_nandita_hmp_data/patric_genomes.txt

while read PATRIC; do 
    cp /pollard/shattuck0/snayfach/databases/PATRIC/genomes/${PATRIC}/${PATRIC}.PATRIC.spgene.tab.gz /pollard/home/ngarud/ben_nandita_hmp_data/patric_spgene
done < ~/ben_nandita_hmp_data/patric_genomes.txt

nohup nice bash ~/ben_nandita_hmp_scripts/bash_scripts/move_spgenes.sh &



while read species; do
nohup nice python ~/ben_nandita_hmp_scripts/plot_spgenes_distribution.py $species --chunk-size 100000 &
done <~/tmp_intermediate_files/tmp_species_list_top30.txt 


########################
# May 5, 2017
# Rerun midas using the 6 differnt reference genomes.
# use bowtie mapping threshold of 94%
########################                                                              
# Goal: replace the following two files genome.features.gz  genome.fna.gz in /pollard/home/ngarud/midas_db/rep_genomes/Bacteroides_uniformis_57318
                                                                                      # First, identify which are the 7 different genomes

cat /pollard/home/ngarud/midas_db/genome_info.txt | grep Bacteroides_uniformis| cut -f1
1235787.3
1339348.3
411479.10
457393.3
585543.3
997889.3
997890.3

# create a new midas DB for each reference genome

for genome_id in 1235787.3 1339348.3 457393.3 585543.3 997889.3 997890.3; do
    nohup nice cp -R ~/ben_nandita_hmp_data/midas_db ~/BenNanditaProject/MIDAS_ref_genome_test/midas_db_new_ref_genomes/midas_db_${genome_id} &
done


# create genome_features files
python ~/ben_nandita_hmp_scripts/create_genome_features_file.py Bacteroides_uniformis_57318

# copy the new genomes in PATRIC  and the new genome_features files   
for genome_id in 1235787.3 1339348.3 457393.3 585543.3 997889.3 997890.3; do
    nohup nice cp /pollard/shattuck0/snayfach/databases/PATRIC/genomes/${genome_id}/${genome_id}.fna.gz ~/BenNanditaProject/MIDAS_ref_genome_test/midas_db_new_ref_genomes/midas_db_${genome_id}/rep_genomes/Bacteroides_uniformis_57318/genome.fna.gz &

    nohup nice cp ~/BenNanditaProject/MIDAS_ref_genome_test/genome_features_files/${genome_id}_features.gz ~/BenNanditaProject/MIDAS_ref_genome_test/midas_db_new_ref_genomes/midas_db_${genome_id}/rep_genomes/Bacteroides_uniformis_57318/genome.features.gz &
done

# run MIDAS
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_snps_diff_ref_genomes

# create bai file. 
for genome in 1235787.3 1339348.3 457393.3 585543.3 700037453 997889.3 997890.3; do 
    cd /pollard/home/ngarud/BenNanditaProject/MIDAS_ref_genome_test/MIDAS_output/${genome}/700037453/snps/temp
    nohup nice samtools index genomes.bam  genomes.bam.bai &
done

nohup nice samtools sort pangenomes.bam pangenomes.sorted.bam &
nohup nice samtools index pangenomes.sorted.bam.bam pangenomes.sorted.bam.bam.bai &
##################################### 
# Test the read stealing hypothesis #
#####################################  
# run MIDAS with just one species
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_snps_B_unif_only

# Run the bowtie converter
for sample in 700037453  700037539; do 
    cd ~/BenNanditaProject/MIDAS_intermediate_files_hmp/MIDAS_1.2.2_read_stealing_test/${sample}/snps/temp
    nohup nice samtools index genomes.bam  genomes.bam.bai &
done


###################################
# CNV analysis
##################################

# make a table assessing for each gene the number of reads mapping for each person. 
while read species; do
nohup nice python ~/ben_nandita_hmp_scripts/count_reads_per_ref_genome.py $species --chunk-size 100000 &
done <~/tmp_intermediate_files/tmp_species_list_top30.txt 

while read species; do
nohup nice python ~/ben_nandita_hmp_scripts/plot_pooled_sfs.py $species &
done <~/tmp_intermediate_files/tmp_species_list_top30.txt 


while read species; do
nohup nice python ~/ben_nandita_hmp_scripts/plot_clade_statistics.py $species --chunk-size 100000 &
done <~/tmp_intermediate_files/tmp_species_list_top30.txt 


############################################################################################
# May 24, 2017
# Blast the first few lines of the single reads files to see if these are 16S sequences.
############################################################################################       

# grab a test file that is single end. Convert it to fasta.

dir=/pollard/home/ngarud/shattuck/metagenomic_fastq_files/Li_2016_FMT
cd $dir
test_file=${dir}/fastq_files/ERR1297575_1.fastq.gz                                                                                                  
gunzip -c $test_file | paste - - - - | cut -f 1,2 | sed 's/^/>/' | tr "\t" "\n" > ${dir}/blast_test/ERR1297575_1.fasta

# get a data set of unaligned 16S green genes DNA
cd blast_test
wget http://greengenes.lbl.gov/Download/Sequence_Data/Fasta_data_files/current_GREENGENES_gg16S_unaligned.fasta.gz

# convert to fasta
gunzip ${dir}/blast_test/current_GREENGENES_gg16S_unaligned.fasta.gz

# Make a database with 16S sequences
makeblastdb -in ${dir}/blast_test/current_GREENGENES_gg16S_unaligned.fasta -out ${dir}/blast_test/GG_16S_db -dbtype nucl

# Run blast

nohup nice blastn -db ${dir}/blast_test/GG_16S_db -query ${dir}/blast_test/ERR1297575_1.fasta -outfmt 6 -out ${dir}/blast_test/ERR1297575_1_blast_output.txt &


############
# Kegg to do:
# 1) CNV analysis
# 2) some bootstrapping to figure out if the outliers are outliers
# 3) boxplot treating each species as a point. 
# 4) Run more species
# 5) Merge core + var genes -- do the results change?
# 6) Do the same genes show up in multiple conserved pathways?
# 7) Drop min no. genes required. 
# 8) draw a species tree



# Look into gene changes properties in terms of CNVs and Kegg pathway:
while read species; do
nohup nice python ~/ben_nandita_hmp_scripts/gene_differences_kegg_annotations.py $species --chunk-size 100000 &
done <~/tmp_intermediate_files/tmp_species_list.txt 

############################################################################
# Blast within-host gene changes against other genomes. Is there a match?  #
############################################################################

###############################################################################
# First make a database that includes every species with sufficient coverage  #
###############################################################################  
# need a database against which I should map to. Get the list of species with enough data 
ls ~/ben_nandita_hmp_data/genes > ~/tmp_intermediate_files/list_of_species_with_gene_data.txt

rm ~/tmp_intermediate_files/all_species_pan_genomes_genes.fasta
while read species_name; do
    zcat ~/ben_nandita_hmp_data/midas_db/pan_genomes/${species_name}/centroids.ffn.gz >> ~/tmp_intermediate_files/all_species_pan_genomes_genes.fasta
done < ~/tmp_intermediate_files/list_of_species_with_gene_data.txt

#make the db
makeblastdb -in ~/tmp_intermediate_files/all_species_pan_genomes_genes.fasta -out ~/tmp_intermediate_files/all_species_pan_genomes_genes_db -dbtype nucl


#################################################
# run blast against the genes that are changing #
#################################################
species=Bacteroides_uniformis_57318
#species=Alistipes_putredinis_61533

# list of genes that show within host chagnes
file=~/ben_nandita_hmp_analysis/${species}_within_host_gene_changes.txt

# need to get the sequences of the genes that are changing. Grep the gene names and reads from this file. 
rm ~/tmp_intermediate_files/${species}_within_host_gene_changes.fasta 
while read gene; do
    samtools faidx ~/ben_nandita_hmp_data/midas_db/pan_genomes/${species}/centroids.ffn.gz $gene >> ~/tmp_intermediate_files/${species}_within_host_gene_changes.fasta
done < $file


# Run blast
blastn -db ~/tmp_intermediate_files/all_species_pan_genomes_genes_db  -query ~/tmp_intermediate_files/${species}_within_host_gene_changes.fasta -outfmt 6 -out ~/tmp_intermediate_files/${species}_all_within_host_gene_changes_blast.txt 

# check which species' genomes that the genes are mapping to. Are they random or closely related genomes?
rm ~/tmp_intermediate_files/${species}_all_within_host_changes_matching_genome.txt 
while read line; do
    gene=`echo $line | cut -f1 -d' '`
    first_no=`echo $line | cut -f2 -d' '| cut -f1 -d'.'`
    second_no=`echo $line | cut -f2 -d' '| cut -f2 -d'.'`
    percent=`echo $line | cut -f3 -d' '`
    bit_score=`echo $line | cut -f12 -d' '`
    matching_genome=`cat ~/ben_nandita_hmp_data/midas_db/genome_info.txt | grep -w ${first_no}.${second_no} | cut -f6`
    echo -e "$gene\t$matching_genome\t$percent\t$bit_score" >> ~/tmp_intermediate_files/${species}_all_within_host_changes_matching_genome.txt
done < ~/tmp_intermediate_files/${species}_all_within_host_gene_changes_blast.txt



#####################################################################################
# repeat for a random list of genes from the pangenome of the species of interest.  #
#####################################################################################
number_genes=`wc -l $file | cut -f1 -d' '`
zcat ~/ben_nandita_hmp_data/midas_db/pan_genomes/${species}/centroids.ffn.gz | grep '>' | shuf -n $number_genes | cut -f2 -d'>' > ~/tmp_intermediate_files/${species}_random_gene_set.txt

# need to get the sequences of the random genes

rm ~/tmp_intermediate_files/${species}_random_gene_set.fasta 
while read gene; do
    samtools faidx ~/ben_nandita_hmp_data/midas_db/pan_genomes/${species}/centroids.ffn.gz $gene >> ~/tmp_intermediate_files/${species}_random_gene_set.fasta
done < ~/tmp_intermediate_files/${species}_random_gene_set.txt


# Run blast on the random gene set 
blastn -db ~/tmp_intermediate_files/all_species_pan_genomes_genes_db  -query ~/tmp_intermediate_files/${species}_random_gene_set.fasta -outfmt 6 -out ~/tmp_intermediate_files/${species}_all_random_gene_set_blast.txt 

# check which species' genomes that the genes are mapping to. Are they random or closely related genomes?
rm ~/tmp_intermediate_files/${species}_all_random_gene_set_matching_genome.txt
while read line; do
    gene=`echo $line | cut -f1 -d' '`
    first_no=`echo $line | cut -f2 -d' '| cut -f1 -d'.'`
    second_no=`echo $line | cut -f2 -d' '| cut -f2 -d'.'`
    percent=`echo $line | cut -f3 -d' '`
    bit_score=`echo $line | cut -f12 -d' '`
    matching_genome=`cat ~/ben_nandita_hmp_data/midas_db/genome_info.txt | grep -w ${first_no}.${second_no} | cut -f6`
    echo -e "$gene\t$matching_genome\t$percent\t$bit_score" >> ~/tmp_intermediate_files/${species}_all_random_gene_set_matching_genome.txt
done < ~/tmp_intermediate_files/${species}_all_random_gene_set_blast.txt


# Python script: 
# quantify the number of species to which the gene can map to with 100% accuracy. 

python ~/ben_nandita_hmp_scripts/find_orthologs.py $species

# run this for all species

while read species; do
    nohup nice bash ~/ben_nandita_hmp_scripts/bash_scripts/find_matching_genes_blast.sh $species &
done <~/tmp_intermediate_files/tmp_species_list.txt 

while read species; do
nohup nice python ~/ben_nandita_hmp_scripts/find_orthologs.py $species &
done  <~/tmp_intermediate_files/tmp_species_list.txt 

#############
# plot all points together
#
python ~/ben_nandita_hmp_scripts/find_orthologs_multispecies.py

##########################################
# check: are two genes truly identical?  #
##########################################
species=Bacteroides_uniformis_57318
gene=1339348.3.peg.2126
samtools faidx ~/ben_nandita_hmp_data/midas_db/pan_genomes/${species}/centroids.ffn.gz $gene

species2=Bacteroides_fragilis_56548
gene2=1073386.3.peg.2910
samtools faidx ~/ben_nandita_hmp_data/midas_db/pan_genomes/${species2}/centroids.ffn.gz $gene2

# no -- second gene is way longer. This is a partial match. But, blast doesn't output stats for length of second gene. 

species=Bacteroides_uniformis_57318
gene=997889.3.peg.1113      
samtools faidx ~/ben_nandita_hmp_data/midas_db/pan_genomes/${species}/centroids.ffn.gz $gene

species2=Bacteroides_vulgatus_57955 
gene2=556260.3.peg.2941
samtools faidx ~/ben_nandita_hmp_data/midas_db/pan_genomes/${species2}/centroids.ffn.gz $gene2

# however this gene does match exactly. 


######################################################
# Make a plot of some statistic vs piS for our paper #
######################################################
python ~/ben_nandita_hmp_scripts/statistics_correlated_with_piS.py 




#########################################################
# Blast against the reference genomes marker genes      #
#########################################################

###############################################################################
# First make a database that includes every species with sufficient coverage  #
###############################################################################  
# need a database against which I should map to. Get the list of species with enough data 
ls ~/ben_nandita_hmp_data/genes > ~/tmp_intermediate_files/list_of_species_with_gene_data.txt

rm ~/tmp_intermediate_files/all_species_pan_genomes_genes.fasta
while read species_name; do
    zcat ~/ben_nandita_hmp_data/midas_db/pan_genomes/${species_name}/centroids.ffn.gz >> ~/tmp_intermediate_files/all_species_pan_genomes_genes.fasta
done < ~/tmp_intermediate_files/list_of_species_with_gene_data.txt

#make the db
makeblastdb -in ~/tmp_intermediate_files/all_species_pan_genomes_genes.fasta -out ~/tmp_intermediate_files/all_species_pan_genomes_genes_db -dbtype nucl

#########################################################################################################
# Do genes that get lost within hosts for the full HMP data set  not get lost when run on only B. unif?
######################################################################################################### 
# rerun this code, but compare the list with the new changes found in the new data set. 
nohup nice python ~/ben_nandita_hmp_scripts/gene_differences_kegg_annotations.py $species --chunk-size 100000 &

# which genes are changing between the two runs? 
Bacteroides_uniformis_57318_within_host_gene_changes_B_unif_only.txt
Bacteroides_uniformis_57318_within_host_gene_changes.txt

Alistipes_putredinis_61533_within_host_gene_changes_A_put_only.txt
Alistipes_putredinis_61533_within_host_gene_changes.txt


# For those genes that are changing, visualize them in IGV if they share the same reference genome

python ~/ben_nandita_hmp_scripts/gene_differences_reference_genome_change.py $species

# see this file:
~/ben_nandita_hmp_analysis/${species}_within_host_gene_changes_database_diffs.txt



#############################################################################################
#
# July 12, 2017
# Change the MIDAS database
# if there is any genome from the HMP project, swap the MIDAS ref genome with the HMP one. 
#
#############################################################################################

# 1. Get the species genome IDS from the MIDAS DB genome_info.txt file
~/midas_db/genome_info.txt

# 2. Look up the source of each genome in the PATRIC genome_metadata file on Pollard server (downloaded by Stephen Nayfach on Nov 14  2015). Determine if genome is HMP reference.
/pollard/shattuck0/snayfach/databases/PATRIC/metadata/genome_metadata

# list of HMP reference genomes (313, not always for unique species). 
/pollard/home/ngarud/ben_nandita_hmp_analysis/list_of_HMP_reference_genomes.txt

cat /pollard/shattuck0/snayfach/databases/PATRIC/metadata/genome_metadata | grep 'Reference genome for the Human Microbiome Project' | cut -f2

python replace_rep_genome_with_HMP.py

# also update genome_info.txt with the refdb used (this will be helpful for the supplement)

#############################################################################################
#
# July 12, 2017
# Time pairs: run the SNP and gene module using the union of species in time pairs
#
#############################################################################################

# 1: get the time pairs
# 2: Get the species list for each time pair in the MIDAS output
# 3: Write a comma-delimited list of species for each sample
# 4: set up commands for running MIDAS with this comma-delimited list

time_pair_species_list_for_midas.py

#parse_midas_data changes:

def parse_intermediate_species_file(sample_id):
def genome_ids_dictionary():
def parse_midas_data.collapse_visno_reps_subject_sample_time_map

# parse patric changes:
def get_HMP_reference_genomes():


# so far I have been editing 
# run MIDAS (SNPs and Genes):

#combined tech reps
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_snps_tech_combined_newDB_timepts_matched
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_genes_tech_combined_newDB_timepts_matched
# combined samples
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_snps_samples_combined_newDB_timepts_matched
qsub ~/ben_nandita_hmp_scripts/qsub_scripts/qsub_script_HMP_MIDAS_genes_samples_combined_newDB_timepts_matched

# repeat for Qin et al. 



while read file; do
    echo $file
    mv /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_Qin_2012/MIDAS_1.2.2_output/${file}/snps /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_Qin_2012/MIDAS_1.2.2_output/${file}/snps_original_db 

    mv /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_Qin_2012/MIDAS_1.2.2_output/${file}/genes /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_Qin_2012/MIDAS_1.2.2_output/${file}/genes_original_db 

done <~/shattuck/BenNanditaProject/MIDAS_intermediate_files_Qin_2012/sample_IDs_healthy.txt


# remove the crassphage file from the genes folder from all 420 outputs because this throws an error when merging genes. 

while read path; do
    mv ${path}/genes/output/crassphage.genes.gz ${path}/genes/crassphage.genes.gz
done < /netapp/home/ngarud/shattuck/BenNanditaProject/MIDAS_intermediate_files_Qin_2012/sample_paths_hmp_Kuleshov_Qin.txt

# also edit the output files for species.txt and summary.txt in the genes intermediate folder to exclude crassphage 


#############################################################################################
#
# July 27, 2017
# How to run post-midas analysis code
#
#############################################################################################

# First postprocess the midas data (takes a 2-3 days on my desktop)
python postprocess_all_midas_data_serial.py 

# Then generate the figures included in the paper
python generate_all_figures.py 



#####################################################################################################
# August 28, 2017
# Pull out the gene names from patric so that Ben and I have the same, but not large-data size copy. 
#####################################################################################################


# run the gene annotation script in parallel on good_species_list:

while read species; do

    nohup nice python ~/ben_nandita_hmp_scripts/gene_changes_annotation.py --other-species $species &

done < ~/tmp_intermediate_files/tmp_species_list.txt
